{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "class MLP:\n",
    "    \n",
    "    def __init__(self,*neurons):\n",
    "        self.layers = len(neurons)\n",
    "        self.neuPerL = [n for n in neurons]\n",
    "    \n",
    "    def setWeights(self,init):\n",
    "        if init == 'SND':\n",
    "            self.weights = [float(np.random.randn(1)) for l in range(self.layers-1)]\n",
    "        elif init == 'Uniform':\n",
    "            self.weights = [float(np.random.rand(1)) for l in range(self.layers-1)]\n",
    "        elif init == 'LeCun':\n",
    "            self.weights = [float(np.random.normal(0,1/np.sqrt(self.neuPerL[l]),1)) for l in range(self.layers-1)]\n",
    "        self.lastChanges = [0 for k in self.weights]",
    "  \n",
    "              \n",
    "    def tanh(self,x):\n",
    "        #return 1/(1+np.exp(-x))\n",
    "        return 1.7159 * np.tanh(2/3 * x)\n",
    "            \n",
    "    def der(self,x):\n",
    "        #return np.exp(x)/(np.exp(x)+1)**2\n",
    "        return 1.14393 * 1/np.cosh(2*x/3)**2\n",
    "    \n",
    "    def forward(self,inp):\n",
    "        self.activation = [inp]\n",
    "        for l in range(self.layers-1):\n",
    "            self.activation.append(self.tanh(np.dot(self.activation[-1],self.weights[l])))       \n",
    "             \n",
    "    def sgd(self,LR,epochs,mom,data,targ):\n",
    "        # Implements Stochastic Gradient Descent\n",
    "        self.LR = LR\n",
    "        self.mom = mom\n",
    "        p = np.zeros([epochs+1])\n",
    "        p[0] = self.test(data,targ)\n",
    "        for epoch in range(epochs):\n",
    "            for sample,target in zip(data,targ):\n",
    "                self.forward(sample)   \n",
    "                # Compute errors\n",
    "                self.deltas = [(self.activation[-1] - target) * self.der(np.dot(self.activation[-2],self.weights[-1]))]\n",
    "                for l in range(len(self.weights)-1):\n",
    "                    self.deltas.append(self.der(np.dot(self.activation[-3-l],self.weights[-2-l])) * self.weights[-1-l]*self.deltas[-1-l])\n",
    "                self.deltas = list(reversed(self.deltas))\n",
    "                \n",
    "                # Adapt weights\n",
    "                self.adaptWeights()   \n",
    "            #self.LR = 0.9 * self.LR # Decaying LR\n",
    "            p[epoch+1] = self.test(data,targ)\n",
    "        return p\n",
    "            \n",
    "    def batch_gd(self,LR,epochs,mom,data,targets):\n",
    "        # Implements Batch Gradient Descent\n",
    "        p = np.zeros([epochs+1])\n",
    "        self.mom = mom\n",
    "        self.LR = LR\n",
    "        for epoch in range(epochs):\n",
    "            activity = np.zeros([len(data),self.layers])\n",
    "            for ind,sample in enumerate(data):\n",
    "                self.forward(sample) \n",
    "                activity[ind,:] = self.activation\n",
    "            loss = np.mean(activity[:,-1] - targets) \n",
    "            self.deltas = [loss * self.der(np.mean(activity[:,-2])*self.weights[-1])]\n",
    "            for l in range(len(self.weights)-1):\n",
    "                self.deltas.append(self.der(np.mean(activity[:,-3-l])*self.weights[-2-l]) * self.weights[-1-l]*self.deltas[-1-l])\n",
    "            self.adaptWeights()\n",
    "            p[epoch+1] = self.test(data,targets)\n",
    "        return p\n",
    "    \n",
    "    def adaptWeights(self):\n",
    "        for l in range(len(self.weights)):\n",
    "            tmp = self.LR * self.deltas[l] * self.activation[l]  + self.mom * self.lastChanges[l]\n",
    "            self.weights[l] = self.weights[l] - tmp\n",
    "            self.lastChanges[l] = tmp\n",
    "\n",
    "    def test(self,data,targ):\n",
    "        correct = 0\n",
    "        for sample,target in zip(data,targ):\n",
    "            self.forward(sample)\n",
    "            correct += 1 if round(self.activation[-1]) == target else 0\n",
    "        return 100*correct/len(data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate and normalize Train Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sampleSize = 30\n",
    "np.random.seed(1)\n",
    "cats = np.random.normal(25,5,sampleSize)\n",
    "dogs = np.random.normal(55,15,sampleSize)\n",
    "\n",
    "\n",
    "data = np.append(cats,dogs)\n",
    "data = (data-np.mean(data)) / np.std(data)\n",
    "t_c = -1 * np.ones([sampleSize])\n",
    "t_d = 1 * np.ones([sampleSize])\n",
    "targets = np.append(t_c,t_d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set Model Hyperparamter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "weight_init = ['Uniform','SND', 'LeCun']\n",
    "LR = 0.3\n",
    "epochs = 100\n",
    "decay = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Call this Method to evaluate the performance of the network\n",
    "# Call it with True adds Momentum, calling it with False leaves it out\n",
    "def evaluate(mom):\n",
    "    perf = np.zeros([2*len(weight_init),epochs+1])\n",
    "    for ind,w in enumerate(weight_init):\n",
    "        # SGD\n",
    "        net = MLP(1,1,1)\n",
    "        net.setWeights(w)\n",
    "        perf[ind,:] = net.sgd(LR,epochs,mom,data,targets)\n",
    "        plt.plot(np.arange(perf.shape[1]),perf[ind,:],label=['SGD',w])\n",
    "\n",
    "        # Batch\n",
    "        net = MLP(1,1,1)\n",
    "        net.setWeights(w)\n",
    "        perf[ind+len(weight_init),:] = net.batch_gd(LR,epochs,mom,data,targets)\n",
    "        plt.plot(np.arange(perf.shape[1]),perf[ind+len(weight_init),:],label=['Batch',w])\n",
    "    plt.legend(loc='upper center', bbox_to_anchor=(0.5, 1.16),ncol=3, fancybox=True, shadow=True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "evaluate(0) # Evaluate without Momentum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "evaluate(decay) # Evaluate with Momentum"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
